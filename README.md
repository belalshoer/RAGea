# RAGea
![](assets/pipeline.png)
In this project, we aim to explore the possibility of improving the performance of Pangea, a recent large multilingual vision-language model, trained to
perform various vision-language tasks, on image captioning by using retrieval
augmented generation. We propose to test whether augmenting generation
with captions from a database of similar images to the input image could
lead to better results. We will compare different retrieval methods, such as
standard nearest neighbor search and maximal matching.

